### Repository of the paper:

## Hide-and-Seek: A template for explainable AI
### by Thanos Tagaris and Andreas Stafylopatis

Abstract:
> Recent breakthroughs in Deep Learning have led to a meteoric rise in the popularity of Neural Networks 
in the scientific world. Their adoption in various sectors of industry, however, has been much slower. 
This can mainly be attributed to the lack of transparency in Neural Networks and the lack of trust this 
entails to their end users. A prevailing disposition regarding interpretability in AI is the so-called 
\textit{Fidelity-Interpretability tradeoff}, which implies that there is a tradeoff between the performance 
of a model and its interpretability. This study explores a framework for creating explainable Neural Networks, 
through the collaborative training of two networks, one trained for classification and one aiming at increasing 
its interpretability. The goal of this technique is to train a model with the highest possible degree of 
interpretability without sacrificing its performance. The experimental study involved image classification 
on 3 datasets: MNIST, CIFAR10 and an ImageNet derivative. Results prove that interpretable Convolutional 
Neural Networks could be trained on all the aforementioned datasets, without reduction in performance. 
Furthermore, evidence indicates that the aforementioned tradeoff is not as steep as related research might 
suggest. 

## Quick start:

## Requirements:

## Idea and Theory:

## Description of experiments and results

## Detailed Guide :

